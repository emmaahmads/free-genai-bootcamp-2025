services:
  # vllm-cpu:
  #   image: superumi/vllm-cpu:latest
  #   container_name: vllm-cpu
  #   ports:
  #     - "8000:8000"
  #   restart: always
  #   environment:
  #     HF_TOKEN: ${HF_TOKEN}
  #     VLLM_CPU_DISABLE_AVX512: "true"
  #     VLLM_CPU_KVCACHE_SPACE: "2"
  #   # command: ["python3", "-m", "vllm.entrypoints.api_server", "--model", "google/gemma-2-9b", "--dtype", "bfloat16"]
  #   # models available: facebook/opt5, mesolitica/mallam-1.1B-4096, openai-community/gpt2
  #   command: ["--model", "mistralai/Mistral-7B-Instruct-v0.1", "--dtype", "bfloat16", "--swap-space", "2", "--disable-frontend-multiprocessing", "--model-impl", "vllm", "--tokenizer-mode", "mistral"] #, "--enable-lora"]

  ollama-server:
    image: ollama/ollama
    container_name: ollama-server
    ports:
      - ${LLM_ENDPOINT_PORT:-11434}:11434
    ipc: host
    restart: unless-stopped
    volumes:
      - ollama:/root/.ollama

  opea-nginx-server:
    image: opea/nginx:latest
    container_name: opea-nginx-server
    ports:
      - "8080:80"
    ipc: host
    restart: always
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro  # Mount NGINX config
    depends_on:
      #- vllm-cpu
      - ollama-server
      - whisper-service
      - speecht5-service
    networks:
      - default
  
  whisper-service:
    image: ${REGISTRY:-opea}/whisper:${TAG:-latest}
    container_name: whisper-service
    ports:
      - "7066:7066"
    ipc: host
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
    restart: unless-stopped
  
  # espeak-service:
  #   image: superumi/espeak-malay:latest
  #   container_name: espeak-service
  #   ports:
  #     - "7055:7055"
  #   ipc: host
  #   depends_on:
  #     - opea-nginx-server
  #   environment:
  #     no_proxy: ${no_proxy}
  #     http_proxy: ${http_proxy}
  #     https_proxy: ${https_proxy}
  #   volumes:
  #     - ./output:/output
  #   command: ["-v", "ms+m3", "-w", "/output/output.wav"]
  #   restart: unless-stopped

  speecht5-service:
    image: ${REGISTRY:-opea}/speecht5:${TAG:-latest}
    container_name: speecht5-service
    ports:
      - ${SPEECHT5_PORT:-7055}:7055
    ipc: host
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
    restart: unless-stopped
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:7055/health"]
    #   interval: 10s
    #   timeout: 6s
    #   retries: 18

volumes:
  ollama:

networks:
  default:
    driver: bridge